{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query Pipelines Llamaindex\n",
    "\n",
    "LlamaIndex provides a declarative query API that allows you to chain together different modules in order to orchestrate simple-to-advanced workflows over your data.\n",
    "\n",
    "This is centered around our QueryPipeline abstraction. Load in a variety of modules (from LLMs to prompts to retrievers to other pipelines), connect them all together into a sequential chain or DAG, and run it end2end.\n",
    "\n",
    "So what are the advantages of QueryPipeline?\n",
    "\n",
    "- Express common workflows with fewer lines of code/boilerplate\n",
    "- Greater readability\n",
    "- Greater parity / better integration points with common low-code / no-code solutions (e.g. LangFlow)\n",
    "- [In the future] A declarative interface allows easy serializability of pipeline components, providing portability of pipelines/easier deployment to different systems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.query_pipeline.query import QueryPipeline\n",
    "from llama_index.llms import OpenAI\n",
    "from llama_index.prompts import PromptTemplate\n",
    "from llama_index.storage import StorageContext\n",
    "from llama_index import (\n",
    "    VectorStoreIndex,\n",
    "    ServiceContext,\n",
    "    SimpleDirectoryReader,\n",
    "    load_index_from_storage,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the API Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load the enviroment variables\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1 docs\n"
     ]
    }
   ],
   "source": [
    "reader = SimpleDirectoryReader(\n",
    "    input_files=[\"./data/paul_graham_essay.txt\"]\n",
    ")\n",
    "docs = reader.load_data()\n",
    "print(f\"Loaded {len(docs)} docs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create or rebuild storage and the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if not os.path.exists(\"storage\"):\n",
    "    index = VectorStoreIndex.from_documents(docs)\n",
    "    # save index to disk\n",
    "    index.set_index_id(\"vector_index\")\n",
    "    index.storage_context.persist(\"./storage\")\n",
    "else:\n",
    "    # rebuild storage context\n",
    "    storage_context = StorageContext.from_defaults(persist_dir=\"storage\")\n",
    "    # load index\n",
    "    index = load_index_from_storage(storage_context, index_id=\"vector_index\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chain Together Prompt and LLM\n",
    "\n",
    "In this section we show a super simple workflow of chaining together a prompt with LLM.\n",
    "\n",
    "We simply define chain on initialization. This is a special case of a query pipeline where the components are purely sequential, and we automatically convert outputs into the right format for the next inputs.\n",
    "\n",
    "#### Defining a Sequential Chain\n",
    "\n",
    "Some simple pipelines are purely linear in nature - the output of the previous module directly goes into the input of the next module.\n",
    "\n",
    "Some examples:\n",
    "\n",
    "- prompt -> LLM -> output parsing\n",
    "- prompt -> LLM -> prompt -> LLM\n",
    "- retriever -> response synthesizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try chaining basic prompts\n",
    "prompt_str = \"Please generate related movies to {movie_name}\"\n",
    "prompt_tmpl = PromptTemplate(prompt_str)\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo\")\n",
    "# Define the query pipeline\n",
    "p = QueryPipeline(chain=[prompt_tmpl, llm], verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;38;2;155;135;227m> Running module a3aea295-140f-4182-99e1-d68a8beee6f0 with input: \n",
      "movie_name: The Departed\n",
      "\n",
      "\u001b[0m\u001b[1;3;38;2;155;135;227m> Running module f785a2ed-cbda-4bc6-85a9-51f04986f0b8 with input: \n",
      "messages: Please generate related movies to The Departed\n",
      "\n",
      "\u001b[0massistant: 1. Infernal Affairs (2002) - The Departed is actually a remake of this Hong Kong crime thriller, which follows a similar storyline of undercover cops infiltrating a criminal organization.\n",
      "\n",
      "2. The Town (2010) - Directed by Ben Affleck, this crime drama revolves around a group of bank robbers in Boston and the FBI agent determined to bring them down.\n",
      "\n",
      "3. Heat (1995) - Directed by Michael Mann, this classic crime film features an intense cat-and-mouse game between a skilled detective and a professional thief in Los Angeles.\n",
      "\n",
      "4. American Gangster (2007) - Based on a true story, this crime drama stars Denzel Washington as a Harlem drug lord and Russell Crowe as the detective determined to bring him to justice.\n",
      "\n",
      "5. Training Day (2001) - Denzel Washington won an Academy Award for his role as a corrupt narcotics detective who takes a rookie cop (Ethan Hawke) on a dangerous ride-along through the streets of Los Angeles.\n",
      "\n",
      "6. The Departed (2006) - Although it's the movie you mentioned, it's worth including it in the list for those who haven't seen it. Directed by Martin Scorsese, this crime thriller follows an undercover cop who infiltrates an Irish gang in Boston, while a mole within the police force works to uncover his true identity.\n",
      "\n",
      "7. Donnie Brasco (1997) - Based on a true story, this crime drama stars Johnny Depp as an undercover FBI agent who infiltrates the Mafia, forming a close bond with a mobster played by Al Pacino.\n",
      "\n",
      "8. The Untouchables (1987) - Directed by Brian De Palma, this crime drama tells the story of Eliot Ness (Kevin Costner) and his team of law enforcement agents as they try to bring down Al Capone (Robert De Niro) during the Prohibition era.\n",
      "\n",
      "9. The French Connection (1971) - This gritty crime thriller follows two New York City detectives as they investigate a massive heroin smuggling operation. It won five Academy Awards, including Best Picture.\n",
      "\n",
      "10. Internal Affairs (1990) - Starring Richard Gere and Andy Garcia, this crime thriller explores the corrupt practices within the Los Angeles Police Department as an Internal Affairs officer investigates a fellow officer's suspicious activities.\n"
     ]
    }
   ],
   "source": [
    "output = p.run(movie_name=\"The Departed\")\n",
    "print(str(output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chain multiple Prompts with Streaming\n",
    "\n",
    "The query pipelines have LLM streaming support (simply do as_query_component(streaming=True)). Intermediate outputs will get autoconverted, and the final output can be a streaming output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_str = \"Please generate related movies to {movie_name}\"\n",
    "prompt_tmpl = PromptTemplate(prompt_str)\n",
    "# let's add some subsequent prompts for fun\n",
    "prompt_str2 = \"\"\"\\\n",
    "Here's some text:\n",
    "\n",
    "{text}\n",
    "\n",
    "Can you rewrite this with a summary of each movie?\n",
    "\"\"\"\n",
    "prompt_tmpl2 = PromptTemplate(prompt_str2)\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo\")\n",
    "llm_c = llm.as_query_component(streaming=True)\n",
    "\n",
    "p = QueryPipeline(\n",
    "    chain=[prompt_tmpl, llm_c, prompt_tmpl2, llm_c], verbose=True\n",
    ")\n",
    "# p = QueryPipeline(chain=[prompt_tmpl, llm_c], verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;38;2;155;135;227m> Running module 75059c2a-0656-48ab-a6f1-9c6b21705828 with input: \n",
      "movie_name: The Dark Knight\n",
      "\n",
      "\u001b[0m\u001b[1;3;38;2;155;135;227m> Running module 68f4a780-4fe7-485c-8650-48878708a24d with input: \n",
      "messages: Please generate related movies to The Dark Knight\n",
      "\n",
      "\u001b[0m\u001b[1;3;38;2;155;135;227m> Running module 4df40a5f-b8f2-4c5c-b2bc-05e21242cfec with input: \n",
      "text: <generator object llm_chat_callback.<locals>.wrap.<locals>.wrapped_llm_chat.<locals>.wrapped_gen at 0x0000025C53B82020>\n",
      "\n",
      "\u001b[0m\u001b[1;3;38;2;155;135;227m> Running module 30e10c95-c374-4d9e-b893-f7c1ce3d003d with input: \n",
      "messages: Here's some text:\n",
      "\n",
      "1. Batman Begins (2005)\n",
      "2. The Dark Knight Rises (2012)\n",
      "3. Batman v Superman: Dawn of Justice (2016)\n",
      "4. Man of Steel (2013)\n",
      "5. The Avengers (2012)\n",
      "6. Iron Man (2008)\n",
      "7. Captain Amer...\n",
      "\n",
      "\u001b[0m1. Batman Begins (2005): A young Bruce Wayne becomes Batman to protect Gotham City from the League of Shadows and their leader, Ra's al Ghul.\n",
      "2. The Dark Knight Rises (2012): Batman returns to save Gotham City from the ruthless terrorist Bane, who plans to destroy it.\n",
      "3. Batman v Superman: Dawn of Justice (2016): Batman and Superman clash as they face off against each other, but eventually join forces to stop the villainous Lex Luthor.\n",
      "4. Man of Steel (2013): The origin story of Superman, as he embraces his powers and battles against General Zod, a fellow Kryptonian seeking to destroy Earth.\n",
      "5. The Avengers (2012): Earth's mightiest heroes, including Iron Man, Captain America, and Thor, unite to stop Loki and his alien army from conquering the world.\n",
      "6. Iron Man (2008): Billionaire Tony Stark becomes Iron Man after being kidnapped and builds a high-tech suit to fight against terrorists and protect the innocent.\n",
      "7. Captain America: The Winter Soldier (2014): Captain America teams up with Black Widow and Falcon to uncover a conspiracy within S.H.I.E.L.D. and face the deadly Winter Soldier.\n",
      "8. The Amazing Spider-Man (2012): Peter Parker gains spider-like abilities and becomes Spider-Man, using his powers to battle the Lizard and protect New York City.\n",
      "9. Watchmen (2009): Set in an alternate reality, a group of retired superheroes investigates the murder of one of their own, uncovering a dark conspiracy threatening humanity.\n",
      "10. Sin City (2005): A collection of interconnected stories set in the crime-ridden city of Basin City, featuring corrupt cops, femme fatales, and vigilante justice.\n",
      "11. V for Vendetta (2005): In a dystopian future, a masked vigilante known as V fights against a totalitarian regime and inspires the people to rise up against oppression.\n",
      "12. Blade Runner 2049 (2017): A young blade runner uncovers a long-buried secret that leads him to seek out former blade runner Rick Deckard, while unraveling the mysteries of a future society.\n",
      "13. Inception (2010): A skilled thief enters people's dreams to steal information, but is tasked with planting an idea instead, leading to a complex and mind-bending heist.\n",
      "14. The Matrix (1999): A computer hacker discovers the truth about reality, joining a group of rebels fighting against sentient machines that have enslaved humanity.\n",
      "15. The Crow (1994): A murdered musician is resurrected by a supernatural crow, seeking vengeance against those who killed him and his fiancée."
     ]
    }
   ],
   "source": [
    "output = p.run(movie_name=\"The Dark Knight\")\n",
    "for o in output:\n",
    "    print(o.delta, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chain Together Query Rewriting Workflow (prompts + LLM) with Retrieval\n",
    "\n",
    "Here we try a slightly more complex workflow where we send the input through two prompts before initiating retrieval.\n",
    "\n",
    "- Generate question about given topic.\n",
    "\n",
    "- Hallucinate answer given question, for better retrieval.\n",
    "\n",
    "Since each prompt only takes in one input, note that the QueryPipeline will automatically chain LLM outputs into the prompt and then into the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.postprocessor import CohereRerank\n",
    "\n",
    "# First prompt: generate question regarding topic\n",
    "prompt_str1 = \"Please generate a concise question about Paul Graham's life regarding the following topic {topic}\"\n",
    "prompt_tmpl1 = PromptTemplate(prompt_str1)\n",
    "# Second prompt: use HyDE to hallucinate answer.\n",
    "prompt_str2 = (\n",
    "    \"Please write a passage to answer the question\\n\"\n",
    "    \"Try to include as many key details as possible.\\n\"\n",
    "    \"\\n\"\n",
    "    \"\\n\"\n",
    "    \"{query_str}\\n\"\n",
    "    \"\\n\"\n",
    "    \"\\n\"\n",
    "    'Passage:\"\"\"\\n'\n",
    ")\n",
    "prompt_tmpl2 = PromptTemplate(prompt_str2)\n",
    "\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo\")\n",
    "retriever = index.as_retriever(similarity_top_k=5)\n",
    "p = QueryPipeline(\n",
    "    chain=[prompt_tmpl1, llm, prompt_tmpl2, llm, retriever], verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;38;2;155;135;227m> Running module 6adb7b56-54f3-4564-a6a7-8d512ee8ae34 with input: \n",
      "topic: college\n",
      "\n",
      "\u001b[0m\u001b[1;3;38;2;155;135;227m> Running module 57bd8005-ea8e-40c6-9ea5-e714cc166401 with input: \n",
      "messages: Please generate a concise question about Paul Graham's life regarding the following topic college\n",
      "\n",
      "\u001b[0m\u001b[1;3;38;2;155;135;227m> Running module a901183d-16ab-4786-aace-15bb59d49844 with input: \n",
      "query_str: assistant: How did Paul Graham's college experience shape his career and entrepreneurial mindset?\n",
      "\n",
      "\u001b[0m\u001b[1;3;38;2;155;135;227m> Running module af775f22-0342-4930-89fc-6a14dc37dbc0 with input: \n",
      "messages: Please write a passage to answer the question\n",
      "Try to include as many key details as possible.\n",
      "\n",
      "\n",
      "How did Paul Graham's college experience shape his career and entrepreneurial mindset?\n",
      "\n",
      "\n",
      "Passage:\"\"\"\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[1;3;38;2;155;135;227m> Running module e68186fe-29c2-4fc4-929b-731d4125fc49 with input: \n",
      "input: assistant: Paul Graham's college experience played a pivotal role in shaping his career and entrepreneurial mindset. As a student at Cornell University, Graham immersed himself in the world of compute...\n",
      "\n",
      "\u001b[0m"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nodes = p.run(topic=\"college\")\n",
    "len(nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Full RAG Pipeline as a DAG\n",
    "\n",
    "Here we chain together a full RAG pipeline consisting of query rewriting, retrieval, reranking, and response synthesis.\n",
    "\n",
    "Here we can’t use chain syntax because certain modules depend on multiple inputs (for instance, response synthesis expects both the retrieved nodes and the original question). Instead we’ll construct a DAG explicitly, through add_modules and then add_link."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAG Pipeline with Query Rewriting\n",
    "\n",
    "We use an LLM to rewrite the query first before passing it to our downstream modules - retrieval/reranking/synthesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.postprocessor import CohereRerank\n",
    "from llama_index.response_synthesizers import TreeSummarize\n",
    "from llama_index import ServiceContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define modules\n",
    "prompt_str = \"Please generate a question about Paul Graham's life regarding the following topic {topic}\"\n",
    "prompt_tmpl = PromptTemplate(prompt_str)\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo\")\n",
    "retriever = index.as_retriever(similarity_top_k=3)\n",
    "reranker = CohereRerank()\n",
    "summarizer = TreeSummarize(\n",
    "    service_context=ServiceContext.from_defaults(llm=llm)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the query pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define query pipeline\n",
    "p = QueryPipeline(verbose=True)\n",
    "p.add_modules(\n",
    "    {\n",
    "        \"llm\": llm,\n",
    "        \"prompt_tmpl\": prompt_tmpl,\n",
    "        \"retriever\": retriever,\n",
    "        \"summarizer\": summarizer,\n",
    "        \"reranker\": reranker,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we draw links between modules with add_link. add_link takes in the source/destination module ids, and optionally the source_key and dest_key. Specify the source_key or dest_key if there are multiple outputs/inputs respectively.\n",
    "\n",
    "You can view the set of input/output keys for each module through module.as_query_component().input_keys and module.as_query_component().output_keys.\n",
    "\n",
    "Here we explicitly specify dest_key for the reranker and summarizer modules because they take in two inputs (query_str and nodes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "required_keys={'query_str', 'nodes'} optional_keys=set()\n"
     ]
    }
   ],
   "source": [
    "p.add_link(\"prompt_tmpl\", \"llm\")\n",
    "p.add_link(\"llm\", \"retriever\")\n",
    "p.add_link(\"retriever\", \"reranker\", dest_key=\"nodes\")\n",
    "p.add_link(\"llm\", \"reranker\", dest_key=\"query_str\")\n",
    "p.add_link(\"reranker\", \"summarizer\", dest_key=\"nodes\")\n",
    "p.add_link(\"llm\", \"summarizer\", dest_key=\"query_str\")\n",
    "\n",
    "# look at summarizer input keys\n",
    "print(summarizer.as_query_component().input_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;38;2;155;135;227m> Running module prompt_tmpl with input: \n",
      "topic: YC\n",
      "\n",
      "\u001b[0m\u001b[1;3;38;2;155;135;227m> Running module llm with input: \n",
      "messages: Please generate a question about Paul Graham's life regarding the following topic YC\n",
      "\n",
      "\u001b[0m\u001b[1;3;38;2;155;135;227m> Running module retriever with input: \n",
      "input: assistant: What role did Paul Graham play in the founding and development of Y Combinator (YC)?\n",
      "\n",
      "\u001b[0m\u001b[1;3;38;2;155;135;227m> Running module reranker with input: \n",
      "query_str: assistant: What role did Paul Graham play in the founding and development of Y Combinator (YC)?\n",
      "nodes: [NodeWithScore(node=TextNode(id_='15e9d741-37ee-47c6-8585-7f972e71fb9d', embedding=None, metadata={'file_path': 'data\\\\paul_graham_essay.txt', 'file_name': 'paul_graham_essay.txt', 'file_type': 'text/...\n",
      "\n",
      "\u001b[0m\u001b[1;3;38;2;155;135;227m> Running module summarizer with input: \n",
      "query_str: assistant: What role did Paul Graham play in the founding and development of Y Combinator (YC)?\n",
      "nodes: [NodeWithScore(node=TextNode(id_='15e9d741-37ee-47c6-8585-7f972e71fb9d', embedding=None, metadata={'file_path': 'data\\\\paul_graham_essay.txt', 'file_name': 'paul_graham_essay.txt', 'file_type': 'text/...\n",
      "\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "response = p.run(topic=\"YC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paul Graham played a significant role in the founding and development of Y Combinator (YC). He was one of the co-founders of YC and was actively involved in shaping its initial vision and structure. He and his co-founders recognized the need for a new type of investment firm that would provide more comprehensive support to early-stage startups. They wanted to help founders with not just funding but also with other crucial aspects of starting a company, such as legal incorporation and mentorship. Paul Graham's experience as a startup founder himself and his understanding of the challenges faced by founders played a crucial role in shaping YC's approach. He also played a role in selecting and funding the first batch of startups and continued to be actively involved in the growth and development of YC over the years.\n"
     ]
    }
   ],
   "source": [
    "print(str(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can do async too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;38;2;155;135;227m> Running modules and inputs in parallel: \n",
      "Module key: prompt_tmpl. Input: \n",
      "topic: YC\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[1;3;38;2;155;135;227m> Running modules and inputs in parallel: \n",
      "Module key: llm. Input: \n",
      "messages: Please generate a question about Paul Graham's life regarding the following topic YC\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[1;3;38;2;155;135;227m> Running modules and inputs in parallel: \n",
      "Module key: retriever. Input: \n",
      "input: assistant: What role did Paul Graham play in the founding and development of Y Combinator (YC)?\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[1;3;38;2;155;135;227m> Running modules and inputs in parallel: \n",
      "Module key: reranker. Input: \n",
      "query_str: assistant: What role did Paul Graham play in the founding and development of Y Combinator (YC)?\n",
      "nodes: [NodeWithScore(node=TextNode(id_='15e9d741-37ee-47c6-8585-7f972e71fb9d', embedding=None, metadata={'file_path': 'data\\\\paul_graham_essay.txt', 'file_name': 'paul_graham_essay.txt', 'file_type': 'text/...\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[1;3;38;2;155;135;227m> Running modules and inputs in parallel: \n",
      "Module key: summarizer. Input: \n",
      "query_str: assistant: What role did Paul Graham play in the founding and development of Y Combinator (YC)?\n",
      "nodes: [NodeWithScore(node=TextNode(id_='15e9d741-37ee-47c6-8585-7f972e71fb9d', embedding=None, metadata={'file_path': 'data\\\\paul_graham_essay.txt', 'file_name': 'paul_graham_essay.txt', 'file_type': 'text/...\n",
      "\n",
      "\n",
      "\u001b[0mPaul Graham played a significant role in the founding and development of Y Combinator (YC). He was one of the co-founders of YC and was actively involved in shaping its early stages. He and his co-founders initially started YC as an angel firm, providing seed investments and support to startups. They aimed to help founders in the same way they had been helped when they were starting their own company. Paul Graham also played a key role in designing the unique batch model of YC, where a group of startups is funded and given intensive support over a three-month period. He believed that funding startups in batches not only made it more convenient for YC but also addressed the problem of isolation faced by founders. Additionally, Paul Graham was involved in selecting and helping founders, as well as writing essays and working on YC's internal software.\n"
     ]
    }
   ],
   "source": [
    "response = await p.arun(topic=\"YC\")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG Pipeline without Query Rewriting\n",
    "\n",
    "Here we setup a RAG pipeline without the query rewriting step.\n",
    "\n",
    "Here we need a way to link the input query to both the retriever, reranker, and summarizer. We can do this by defining a special InputComponent, allowing us to link the inputs to multiple downstream modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.postprocessor import CohereRerank\n",
    "from llama_index.response_synthesizers import TreeSummarize\n",
    "from llama_index import ServiceContext\n",
    "from llama_index.query_pipeline import InputComponent\n",
    "\n",
    "retriever = index.as_retriever(similarity_top_k=5)\n",
    "summarizer = TreeSummarize(\n",
    "    service_context=ServiceContext.from_defaults(\n",
    "        llm=OpenAI(model=\"gpt-3.5-turbo\")\n",
    "    )\n",
    ")\n",
    "reranker = CohereRerank()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = QueryPipeline(verbose=True)\n",
    "p.add_modules(\n",
    "    {\n",
    "        \"input\": InputComponent(),\n",
    "        \"retriever\": retriever,\n",
    "        \"summarizer\": summarizer,\n",
    "    }\n",
    ")\n",
    "p.add_link(\"input\", \"retriever\")\n",
    "p.add_link(\"input\", \"summarizer\", dest_key=\"query_str\")\n",
    "p.add_link(\"retriever\", \"summarizer\", dest_key=\"nodes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;38;2;155;135;227m> Running module input with input: \n",
      "input: what did the author do in YC\n",
      "\n",
      "\u001b[0m\u001b[1;3;38;2;155;135;227m> Running module retriever with input: \n",
      "input: what did the author do in YC\n",
      "\n",
      "\u001b[0m\u001b[1;3;38;2;155;135;227m> Running module summarizer with input: \n",
      "query_str: what did the author do in YC\n",
      "nodes: [NodeWithScore(node=TextNode(id_='3c623089-a168-40b9-9807-edea40010eb4', embedding=None, metadata={'file_path': 'data\\\\paul_graham_essay.txt', 'file_name': 'paul_graham_essay.txt', 'file_type': 'text/...\n",
      "\n",
      "\u001b[0mThe author worked in various aspects of Y Combinator (YC). They funded startups as an angel investor, provided support and guidance to founders, and helped them with setting up their companies. Additionally, they worked on internal software for YC, wrote essays about startups, and worked on a news aggregator called Hacker News.\n"
     ]
    }
   ],
   "source": [
    "output = p.run(input=\"what did the author do in YC\")\n",
    "print(str(output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining a Custom Component in a Query Pipeline\n",
    "\n",
    "You can easily define a custom component. Simply subclass a QueryComponent, implement validation/run functions + some helpers, and plug it in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.query_pipeline import (\n",
    "    CustomQueryComponent,\n",
    "    InputKeys,\n",
    "    OutputKeys,\n",
    ")\n",
    "from typing import Dict, Any\n",
    "from llama_index.llms.llm import BaseLLM\n",
    "from pydantic import Field\n",
    "\n",
    "\n",
    "class RelatedMovieComponent(CustomQueryComponent):\n",
    "    \"\"\"Related movie component.\"\"\"\n",
    "\n",
    "    llm: BaseLLM = Field(..., description=\"OpenAI LLM\")\n",
    "\n",
    "    def _validate_component_inputs(\n",
    "        self, input: Dict[str, Any]\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"Validate component inputs during run_component.\"\"\"\n",
    "        # NOTE: this is OPTIONAL but we show you here how to do validation as an example\n",
    "        return input\n",
    "\n",
    "    @property\n",
    "    def _input_keys(self) -> set:\n",
    "        \"\"\"Input keys dict.\"\"\"\n",
    "        # NOTE: These are required inputs. If you have optional inputs please override\n",
    "        # `optional_input_keys_dict`\n",
    "        return {\"movie\"}\n",
    "\n",
    "    @property\n",
    "    def _output_keys(self) -> set:\n",
    "        return {\"output\"}\n",
    "\n",
    "    def _run_component(self, **kwargs) -> Dict[str, Any]:\n",
    "        \"\"\"Run the component.\"\"\"\n",
    "        # use QueryPipeline itself here for convenience\n",
    "        prompt_str = \"Please generate related movies to {movie_name}\"\n",
    "        prompt_tmpl = PromptTemplate(prompt_str)\n",
    "        p = QueryPipeline(chain=[prompt_tmpl, llm])\n",
    "        return {\"output\": p.run(movie_name=kwargs[\"movie\"])}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s try the custom component out! We’ll also add a step to convert the output to Shakespeare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(model=\"gpt-3.5-turbo\")\n",
    "component = RelatedMovieComponent(llm=llm)\n",
    "\n",
    "# let's add some subsequent prompts for fun\n",
    "prompt_str = \"\"\"\\\n",
    "Here's some text:\n",
    "\n",
    "{text}\n",
    "\n",
    "Can you rewrite this in the voice of Shakespeare?\n",
    "\"\"\"\n",
    "prompt_tmpl = PromptTemplate(prompt_str)\n",
    "\n",
    "p = QueryPipeline(chain=[component, prompt_tmpl, llm], verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;38;2;155;135;227m> Running module 1ebfa41f-5ac9-441c-b009-b18cf45edec0 with input: \n",
      "movie: Love Actually\n",
      "\n",
      "\u001b[0m\u001b[1;3;38;2;155;135;227m> Running module e9ce341a-1ae8-4379-9d04-790a0ed024f9 with input: \n",
      "text: assistant: 1. \"Valentine's Day\" (2010) - This romantic comedy follows the lives of several interconnected couples and singles in Los Angeles as they navigate love and relationships on Valentine's Day....\n",
      "\n",
      "\u001b[0m\u001b[1;3;38;2;155;135;227m> Running module dd728f26-fcaa-4cad-bd57-fa7ee052936f with input: \n",
      "messages: Here's some text:\n",
      "\n",
      "1. \"Valentine's Day\" (2010) - This romantic comedy follows the lives of several interconnected couples and singles in Los Angeles as they navigate love and relationships on Valentin...\n",
      "\n",
      "\u001b[0massistant: 1. \"Valentine's Daye\" (2010) - Thise romantic comedy doth follow the lives of several interconnected couples and singles in Los Angeles as they doth navigate love and relationships on Valentine's Daye.\n",
      "\n",
      "2. \"New Year's Eve\" (2011) - Similar to \"Love Actually,\" thise film doth tell the story of multiple characters whose lives doth intertwine on New Year's Eve in New York City, exploring themes of love, hope, and second chances.\n",
      "\n",
      "3. \"Crazy, Stupid, Love\" (2011) - Thise movie doth revolve around a middle-aged man who, after his wife doth ask him for a divorce, seeketh the help of a young womanizer to rediscover his confidence and find love again.\n",
      "\n",
      "4. \"The Holiday\" (2006) - In thise heartwarming film, two women from different countries doth swap homes for the holidays and unexpectedly find love in the process. It doth explore themes of self-discovery, second chances, and the magic of the holiday season.\n",
      "\n",
      "5. \"Notting Hill\" (1999) - Starring Hugh Grant and Julia Roberts, thise romantic comedy doth follow the unlikely love story between a British bookstore owner and a famous American actress who doth meet by chance in the charming neighborhood of Notting Hill, London.\n",
      "\n",
      "6. \"Bridget Jones's Diary\" (2001) - Based on the popular novel, thise film doth follow the life of Bridget Jones, a quirky and relatable woman who doth navigate her love life, career, and self-image whilst juggling two potential suitors.\n",
      "\n",
      "7. \"Four Weddings and a Funeral\" (1994) - Thise British romantic comedy doth center around a group of friends who doth attend various weddings and a funeral, exploring themes of love, friendship, and the complexities of relationships.\n",
      "\n",
      "8. \"About Time\" (2013) - From the creators of \"Love Actually,\" thise film doth tell the story of a young man who doth discover he can time travel and useth thise ability to find love and make the most of every moment in his life.\n",
      "\n",
      "9. \"Serendipity\" (2001) - Thise romantic comedy doth follow the story of two strangers who doth meet by chance and feel an instant connection. Despite going their separate ways, they doth believe in fate and spendeth years trying to find each other again.\n",
      "\n",
      "10. \"The Best Exotic Marigold Hotel\" (2011) - A group of British retirees doth decide to spend their golden years in a seemingly luxurious hotel in India. As they doth navigate new friendships, love interests, and cultural differences, they doth discover that life can still hold surprises and love can bloom at any age.\n"
     ]
    }
   ],
   "source": [
    "output = p.run(movie=\"Love Actually\")\n",
    "print(str(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
