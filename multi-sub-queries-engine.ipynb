{"cells":[{"cell_type":"markdown","metadata":{"id":"UU-7NtlCrKFD"},"source":["\n","<a href=\"https://colab.research.google.com/github/edumunozsala/llamaindex-RAG-techniques/blob/main/multi-sub-queries-engine.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"]},{"cell_type":"markdown","metadata":{"id":"_4THJyQNrKFH"},"source":["# Sub Query RetrieverEngine for RAG \n","\n","Query transformations are a family of techniques using an LLM as a reasoning engine to modify user input in order to improve retrieval quality.\n","\n","In this notebook, we showcase how to use a sub question query engine to tackle the problem of answering a complex query using multiple data sources.\n","It first breaks down the complex query into sub questions for each relevant data source, then gather all the intermediate reponses and synthesizes a final response.\n","\n","Original code from Llama-index: https://docs.llamaindex.ai/en/stable/examples/query_engine/sub_question_query_engine.html"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["import os\n","import openai\n","\n","from llama_index import VectorStoreIndex\n","from llama_index.tools import QueryEngineTool, ToolMetadata\n","from llama_index.query_engine import SubQuestionQueryEngine\n","from llama_index.callbacks import CallbackManager, LlamaDebugHandler\n","from llama_index import ServiceContext"]},{"cell_type":"markdown","metadata":{},"source":["Load the API Keys:"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"gIiPgcBGrKFI"},"outputs":[{"data":{"text/plain":["True"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["from dotenv import load_dotenv\n","\n","# Load the enviroment variables\n","load_dotenv()"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["# NOTE: This is ONLY necessary in jupyter notebook.\n","# Details: Jupyter runs an event-loop behind the scenes.\n","#          This results in nested event-loops when we start an event-loop to make async queries.\n","#          This is normally not allowed, we use nest_asyncio to allow it for convenience.\n","import nest_asyncio\n","\n","nest_asyncio.apply()"]},{"cell_type":"markdown","metadata":{"id":"4nSK0AfgrKFJ"},"source":["## Setup"]},{"cell_type":"markdown","metadata":{"id":"HL_YqBQZrKFK"},"source":["\n","If you're opening this Notebook on colab, you will probably need to install LlamaIndex ðŸ¦™."]},{"cell_type":"markdown","metadata":{},"source":["### Load the data"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"py8iLYSNrKFL"},"outputs":[],"source":["from pathlib import Path\n","from llama_index import download_loader\n","\n","PDFReader = download_loader(\"PDFReader\")\n","\n","loader = PDFReader()\n","documents = loader.load_data(file=Path('./data/Attention is all you need.pdf'))"]},{"cell_type":"markdown","metadata":{},"source":["First, we setup a DebugHandler to keep track of the subquestions"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["llama_debug = LlamaDebugHandler(print_trace_on_end=True)\n","\n","callback_manager = CallbackManager([llama_debug])"]},{"cell_type":"markdown","metadata":{"id":"7WOTipxCrKFM"},"source":["Next, we will setup a vector index over the documentation."]},{"cell_type":"code","execution_count":6,"metadata":{"id":"3N8P3WFBrKFM"},"outputs":[{"name":"stdout","output_type":"stream","text":["**********\n","Trace: index_construction\n","    |_CBEventType.NODE_PARSING ->  0.054173 seconds\n","      |_CBEventType.CHUNKING ->  0.002804 seconds\n","      |_CBEventType.CHUNKING ->  0.003553 seconds\n","      |_CBEventType.CHUNKING ->  0.0 seconds\n","      |_CBEventType.CHUNKING ->  0.0 seconds\n","      |_CBEventType.CHUNKING ->  0.013239 seconds\n","      |_CBEventType.CHUNKING ->  0.0 seconds\n","      |_CBEventType.CHUNKING ->  0.0 seconds\n","      |_CBEventType.CHUNKING ->  0.0 seconds\n","      |_CBEventType.CHUNKING ->  0.005479 seconds\n","      |_CBEventType.CHUNKING ->  0.0 seconds\n","      |_CBEventType.CHUNKING ->  0.012442 seconds\n","      |_CBEventType.CHUNKING ->  0.0 seconds\n","      |_CBEventType.CHUNKING ->  0.0 seconds\n","      |_CBEventType.CHUNKING ->  0.0 seconds\n","      |_CBEventType.CHUNKING ->  0.0 seconds\n","    |_CBEventType.EMBEDDING ->  1.215931 seconds\n","    |_CBEventType.EMBEDDING ->  1.215931 seconds\n","    |_CBEventType.EMBEDDING ->  1.215931 seconds\n","    |_CBEventType.EMBEDDING ->  1.215931 seconds\n","**********\n"]}],"source":["# D\n","service_context = ServiceContext.from_defaults(\n","    callback_manager=callback_manager,chunk_size=512\n",")\n","\n","#service_context = ServiceContext.from_defaults(chunk_size=512)\n","\n","# build index and query engine\n","vector_query_engine = VectorStoreIndex.from_documents(\n","    documents, use_async=True, service_context=service_context\n",").as_query_engine()"]},{"cell_type":"markdown","metadata":{"id":"JPmdeN2GrKFM"},"source":["## Setup sub question query engine\n","\n"]},{"cell_type":"markdown","metadata":{"id":"a8GA6GCwrKFN"},"source":["Now, we create our subquery engine based on a new defined tool:"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["# setup base query engine as tool\n","query_engine_tools = [\n","    QueryEngineTool(\n","        query_engine=vector_query_engine,\n","        metadata=ToolMetadata(\n","            name=\"Attention_paper\",\n","            description=\"Attention is all you nedd paper\",\n","        ),\n","    ),\n","]\n","\n","query_engine = SubQuestionQueryEngine.from_defaults(\n","    query_engine_tools=query_engine_tools,\n","    service_context=service_context,\n","    use_async=True,\n",")"]},{"cell_type":"markdown","metadata":{"id":"xNVELxw7rKFP"},"source":["## Use the  Sub Query Engine!\n","\n","Now, we can invoke the sub query engine to synthesize natural language responses."]},{"cell_type":"code","execution_count":8,"metadata":{"id":"PJAxZl3TrKFP","outputId":"41f6d1f7-20c0-48b0-d96e-ca6b172c34b8"},"outputs":[{"name":"stdout","output_type":"stream","text":["Generated 6 sub questions.\n","\u001b[1;3;38;2;237;90;200m[Attention_paper] Q: What is a transformer?\n","\u001b[0m\u001b[1;3;38;2;90;149;237m[Attention_paper] Q: What is a convolutional neural network?\n","\u001b[0m\u001b[1;3;38;2;11;159;203m[Attention_paper] Q: How do transformers work?\n","\u001b[0m\u001b[1;3;38;2;155;135;227m[Attention_paper] Q: How do convolutional neural networks work?\n","\u001b[0m\u001b[1;3;38;2;237;90;200m[Attention_paper] Q: What are the similarities between transformers and convolutional neural networks?\n","\u001b[0m\u001b[1;3;38;2;90;149;237m[Attention_paper] Q: What are the differences between transformers and convolutional neural networks?\n","\u001b[0m\u001b[1;3;38;2;90;149;237m[Attention_paper] A: A convolutional neural network (CNN) is a type of neural network that is commonly used for image recognition and computer vision tasks. It is designed to automatically learn and extract features from input images through a series of convolutional layers. These layers apply filters to the input image, which helps to identify patterns and features at different scales. CNNs have been widely successful in various applications, including object detection, image classification, and image segmentation.\n","\u001b[0m\u001b[1;3;38;2;237;90;200m[Attention_paper] A: The Transformer is a transduction model that relies entirely on self-attention to compute representations of its input and output, without using sequence-aligned recurrent neural networks (RNNs) or convolution. It consists of an encoder-decoder structure, where the encoder maps an input sequence of symbol representations to a sequence of continuous representations, and the decoder generates an output sequence of symbols based on the encoded representations. The Transformer utilizes stacked self-attention and point-wise, fully connected layers for both the encoder and decoder.\n","\u001b[0m\u001b[1;3;38;2;237;90;200m[Attention_paper] A: Transformers and convolutional neural networks (CNNs) both use parallel computation to reduce sequential computation. They both aim to compute hidden representations in parallel for all input and output positions. However, there are differences in the way they handle dependencies between distant positions. Transformers reduce the number of operations required to relate signals from two arbitrary input or output positions to a constant number, while CNNs have a linear or logarithmic growth in the number of operations with the distance between positions. This difference in handling dependencies makes it more difficult for CNNs to learn dependencies between distant positions compared to Transformers.\n","\u001b[0m\u001b[1;3;38;2;11;159;203m[Attention_paper] A: Transformers work by relying entirely on self-attention to compute representations of their input and output, without using sequence-aligned recurrent neural networks (RNNs) or convolution. They have an encoder-decoder structure, where the encoder maps an input sequence to a sequence of continuous representations, and the decoder generates an output sequence based on the encoder's representations. The Transformer architecture uses stacked self-attention and point-wise, fully connected layers for both the encoder and decoder. This allows for more parallelization and reduces the number of operations required to relate signals from different positions, making it easier to learn dependencies between distant positions.\n","\u001b[0m\u001b[1;3;38;2;90;149;237m[Attention_paper] A: Transformers and convolutional neural networks (CNNs) have some key differences. While CNNs use convolutional layers to compute hidden representations in parallel for all input and output positions, transformers rely on self-attention to compute representations of its input and output. \n","\n","In terms of the number of operations required to relate signals from two arbitrary input or output positions, CNNs have a linear growth in distance between positions, while transformers have a constant number of operations. This makes it more difficult for CNNs to learn dependencies between distant positions compared to transformers.\n","\n","Additionally, transformers use stacked self-attention and point-wise, fully connected layers for both the encoder and decoder, while CNNs use convolutional layers as their basic building block.\n","\n","Overall, transformers and CNNs have different approaches to computing representations and handling dependencies between positions, with transformers relying on self-attention and CNNs using convolutional layers.\n","\u001b[0m\u001b[1;3;38;2;155;135;227m[Attention_paper] A: Convolutional neural networks (CNNs) are a type of neural network that are commonly used in computer vision tasks. They are designed to process data with a grid-like structure, such as images. \n","\n","CNNs consist of multiple layers, including convolutional layers, pooling layers, and fully connected layers. The convolutional layers are responsible for learning and extracting features from the input data. They apply a set of learnable filters (also known as kernels) to the input data, performing a convolution operation. This operation helps to detect different patterns and features in the data, such as edges, corners, and textures.\n","\n","Pooling layers are used to reduce the spatial dimensions of the data and help to make the network more robust to variations in the input. They downsample the feature maps generated by the convolutional layers, typically by taking the maximum or average value within a certain region.\n","\n","The output of the convolutional and pooling layers is then flattened and passed through fully connected layers. These layers are similar to those in traditional neural networks and are responsible for making predictions based on the learned features.\n","\n","Overall, CNNs are effective in capturing spatial hierarchies and patterns in data, making them well-suited for tasks such as image classification, object detection, and image segmentation.\n","\u001b[0m**********\n","Trace: query\n","    |_CBEventType.QUERY ->  21.823054 seconds\n","      |_CBEventType.LLM ->  3.959074 seconds\n","      |_CBEventType.SUB_QUESTION ->  4.87324 seconds\n","        |_CBEventType.QUERY ->  4.863376 seconds\n","          |_CBEventType.RETRIEVE ->  0.334472 seconds\n","            |_CBEventType.EMBEDDING ->  0.32393 seconds\n","          |_CBEventType.SYNTHESIZE ->  4.528904 seconds\n","            |_CBEventType.TEMPLATING ->  0.0 seconds\n","            |_CBEventType.LLM ->  4.528904 seconds\n","      |_CBEventType.SUB_QUESTION ->  3.967704 seconds\n","        |_CBEventType.QUERY ->  3.967704 seconds\n","          |_CBEventType.RETRIEVE ->  0.816495 seconds\n","            |_CBEventType.EMBEDDING ->  0.815458 seconds\n","          |_CBEventType.SYNTHESIZE ->  3.151209 seconds\n","            |_CBEventType.TEMPLATING ->  0.0 seconds\n","            |_CBEventType.LLM ->  3.151209 seconds\n","      |_CBEventType.SUB_QUESTION ->  5.505798 seconds\n","        |_CBEventType.QUERY ->  5.505798 seconds\n","          |_CBEventType.RETRIEVE ->  0.844906 seconds\n","            |_CBEventType.EMBEDDING ->  0.844906 seconds\n","          |_CBEventType.SYNTHESIZE ->  4.660892 seconds\n","            |_CBEventType.TEMPLATING ->  0.0 seconds\n","            |_CBEventType.LLM ->  4.649068 seconds\n","      |_CBEventType.SUB_QUESTION ->  9.474982 seconds\n","        |_CBEventType.QUERY ->  9.47398 seconds\n","          |_CBEventType.RETRIEVE ->  0.798101 seconds\n","            |_CBEventType.EMBEDDING ->  0.798101 seconds\n","          |_CBEventType.SYNTHESIZE ->  8.675879 seconds\n","            |_CBEventType.TEMPLATING ->  0.0 seconds\n","            |_CBEventType.LLM ->  8.665354 seconds\n","      |_CBEventType.SUB_QUESTION ->  5.345784 seconds\n","        |_CBEventType.QUERY ->  5.345784 seconds\n","          |_CBEventType.RETRIEVE ->  0.826431 seconds\n","            |_CBEventType.EMBEDDING ->  0.826431 seconds\n","          |_CBEventType.SYNTHESIZE ->  4.519353 seconds\n","            |_CBEventType.TEMPLATING ->  0.0 seconds\n","            |_CBEventType.LLM ->  4.501724 seconds\n","      |_CBEventType.SUB_QUESTION ->  7.024621 seconds\n","        |_CBEventType.QUERY ->  7.024621 seconds\n","          |_CBEventType.RETRIEVE ->  0.859912 seconds\n","            |_CBEventType.EMBEDDING ->  0.859912 seconds\n","          |_CBEventType.SYNTHESIZE ->  6.164709 seconds\n","            |_CBEventType.TEMPLATING ->  0.0 seconds\n","            |_CBEventType.LLM ->  6.164709 seconds\n","      |_CBEventType.SYNTHESIZE ->  8.367518 seconds\n","        |_CBEventType.TEMPLATING ->  0.0 seconds\n","        |_CBEventType.LLM ->  8.363261 seconds\n","**********\n"]}],"source":["response = query_engine.query(\"How are transformers related to convolutional neural networks?\")"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"OPfSdZRbrKFP","outputId":"c9ca099a-a718-4f7f-fe31-4da1ac0ca120"},"outputs":[{"data":{"text/markdown":["**`Final Response:`** Transformers and convolutional neural networks (CNNs) are both types of neural networks used in different domains. While CNNs are commonly used for image recognition and computer vision tasks, transformers are often used in natural language processing and sequence-to-sequence tasks.\n","\n","Both transformers and CNNs aim to compute hidden representations in parallel for all input and output positions. However, there are differences in the way they handle dependencies between distant positions. Transformers reduce the number of operations required to relate signals from two arbitrary input or output positions to a constant number, while CNNs have a linear or logarithmic growth in the number of operations with the distance between positions. This difference in handling dependencies makes it more difficult for CNNs to learn dependencies between distant positions compared to transformers.\n","\n","In terms of architecture, transformers use stacked self-attention and point-wise, fully connected layers for both the encoder and decoder, while CNNs use convolutional layers as their basic building block.\n","\n","Overall, while transformers and CNNs have some similarities in terms of parallel computation, they have different approaches to computing representations and handling dependencies between positions. Transformers rely on self-attention, while CNNs use convolutional layers."],"text/plain":["<IPython.core.display.Markdown object>"]},"metadata":{},"output_type":"display_data"}],"source":["from llama_index.response.notebook_utils import display_response\n","\n","display_response(response)"]},{"cell_type":"markdown","metadata":{},"source":["#### Iterate over the sub queries"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Sub Question 0: What is a transformer?\n","Answer: The Transformer is a transduction model that relies entirely on self-attention to compute representations of its input and output, without using sequence-aligned recurrent neural networks (RNNs) or convolution. It consists of an encoder-decoder structure, where the encoder maps an input sequence of symbol representations to a sequence of continuous representations, and the decoder generates an output sequence of symbols based on the encoded representations. The Transformer utilizes stacked self-attention and point-wise, fully connected layers for both the encoder and decoder.\n","====================================\n","Sub Question 1: What is a convolutional neural network?\n","Answer: A convolutional neural network (CNN) is a type of neural network that is commonly used for image recognition and computer vision tasks. It is designed to automatically learn and extract features from input images through a series of convolutional layers. These layers apply filters to the input image, which helps to identify patterns and features at different scales. CNNs have been widely successful in various applications, including object detection, image classification, and image segmentation.\n","====================================\n","Sub Question 2: How do transformers work?\n","Answer: Transformers work by relying entirely on self-attention to compute representations of their input and output, without using sequence-aligned recurrent neural networks (RNNs) or convolution. They have an encoder-decoder structure, where the encoder maps an input sequence to a sequence of continuous representations, and the decoder generates an output sequence based on the encoder's representations. The Transformer architecture uses stacked self-attention and point-wise, fully connected layers for both the encoder and decoder. This allows for more parallelization and reduces the number of operations required to relate signals from different positions, making it easier to learn dependencies between distant positions.\n","====================================\n","Sub Question 3: How do convolutional neural networks work?\n","Answer: Convolutional neural networks (CNNs) are a type of neural network that are commonly used in computer vision tasks. They are designed to process data with a grid-like structure, such as images. \n","\n","CNNs consist of multiple layers, including convolutional layers, pooling layers, and fully connected layers. The convolutional layers are responsible for learning and extracting features from the input data. They apply a set of learnable filters (also known as kernels) to the input data, performing a convolution operation. This operation helps to detect different patterns and features in the data, such as edges, corners, and textures.\n","\n","Pooling layers are used to reduce the spatial dimensions of the data and help to make the network more robust to variations in the input. They downsample the feature maps generated by the convolutional layers, typically by taking the maximum or average value within a certain region.\n","\n","The output of the convolutional and pooling layers is then flattened and passed through fully connected layers. These layers are similar to those in traditional neural networks and are responsible for making predictions based on the learned features.\n","\n","Overall, CNNs are effective in capturing spatial hierarchies and patterns in data, making them well-suited for tasks such as image classification, object detection, and image segmentation.\n","====================================\n","Sub Question 4: What are the similarities between transformers and convolutional neural networks?\n","Answer: Transformers and convolutional neural networks (CNNs) both use parallel computation to reduce sequential computation. They both aim to compute hidden representations in parallel for all input and output positions. However, there are differences in the way they handle dependencies between distant positions. Transformers reduce the number of operations required to relate signals from two arbitrary input or output positions to a constant number, while CNNs have a linear or logarithmic growth in the number of operations with the distance between positions. This difference in handling dependencies makes it more difficult for CNNs to learn dependencies between distant positions compared to Transformers.\n","====================================\n","Sub Question 5: What are the differences between transformers and convolutional neural networks?\n","Answer: Transformers and convolutional neural networks (CNNs) have some key differences. While CNNs use convolutional layers to compute hidden representations in parallel for all input and output positions, transformers rely on self-attention to compute representations of its input and output. \n","\n","In terms of the number of operations required to relate signals from two arbitrary input or output positions, CNNs have a linear growth in distance between positions, while transformers have a constant number of operations. This makes it more difficult for CNNs to learn dependencies between distant positions compared to transformers.\n","\n","Additionally, transformers use stacked self-attention and point-wise, fully connected layers for both the encoder and decoder, while CNNs use convolutional layers as their basic building block.\n","\n","Overall, transformers and CNNs have different approaches to computing representations and handling dependencies between positions, with transformers relying on self-attention and CNNs using convolutional layers.\n","====================================\n"]}],"source":["# iterate through sub_question items captured in SUB_QUESTION event\n","from llama_index.callbacks.schema import CBEventType, EventPayload\n","\n","for i, (start_event, end_event) in enumerate(\n","    llama_debug.get_event_pairs(CBEventType.SUB_QUESTION)\n","):\n","    qa_pair = end_event.payload[EventPayload.SUB_QUESTION]\n","    print(\"Sub Question \" + str(i) + \": \" + qa_pair.sub_q.sub_question.strip())\n","    print(\"Answer: \" + qa_pair.answer.strip())\n","    print(\"====================================\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[{"file_id":"https://github.com/run-llama/llama_index/blob/main/docs/examples/retrievers/reciprocal_rerank_fusion.ipynb","timestamp":1703234328667}]},"kernelspec":{"display_name":"llama-index-4a-wkI5X-py3.11","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.6"}},"nbformat":4,"nbformat_minor":0}
